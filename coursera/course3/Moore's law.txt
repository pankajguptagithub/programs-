Moore’s Law is a computing term which originated around 1970; the simplified version of this law
states that processor speeds, or overall processing power for computers will double every two years.
The term is not very popular but the rule is still accepted. To break down the law even further, it
specifically stated that the number of transistors on an affordable CPU would double every two years
but ‘more transistors’ is more accurate.
If we look at processor speeds from the 1970’s to 2009 and then again in 2010, one may think that the
law has reached its limit or is nearing the limit. In the 1970’s processor speeds ranged from 740 KHz to
8MHz. From 2000 – 2009 there has not really been much of a speed difference as the speeds range
from 1.3 GHz to 2.8 GHz, which suggests that the speeds have barely doubled within a 10 year span.
This is because we are looking at the speeds and not the number of transistors; in 2000 the number of
transistors in the CPU numbered 37.5 million, while in 2009 the number went up to an outstanding 904
million; this is why it is more accurate to apply the law to transistors than to speed.
The reasons for which this law stopped being true are:
1. As transistors increase, power demand increases, which increases heat.
2. Smaller transistors switch faster
3. Exponential increase in density would lead to exponential increase in speed
4. Transistor’s need a minimum voltage to switch, and voltage reduction has lower limits due to
noise.
5. Dynamic power consumption is reduced by voltage scaling.
6. Voltage scaling does not prevent power leakage.
7. Temperature increases as power increases.
8. Power increases as transistor density increases.
9. Voltage scaling reduces (dynamic) power consumption.
10. Voltage scaling cannot prevent leakage power loss.
11. Voltage scaling is limited due to noise or threshold voltage.
